{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2eb02e6-919b-49a0-b4ac-01a9eb21604c",
   "metadata": {},
   "source": [
    "## qwen2.5推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b133362a-546e-45bd-9685-fba444c23507",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e86b56b1-0e0a-4f29-b898-055f51cad0bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure! A large language model (LLM) is a type of artificial intelligence that can generate human-like text based on the input it receives. These models are designed to mimic natural language understanding and generation capabilities, making them useful for a variety of applications such as chatbots, virtual assistants, language translation, and more.\\n\\nLarge language models are typically composed of multiple layers or \"layers\" of neural networks, each responsible for processing different aspects of language understanding and generation. This allows these models to learn from vast amounts of data and improve their performance over time.\\n\\nThe term \"large\" refers to the fact that these models have been trained using massive datasets, which means they have access to an enormous amount of information about language patterns and contexts. The size of these datasets also influences the complexity and effectiveness of the models\\' ability to understand and generate human-like language.\\n\\nIn summary, large language models are powerful AI tools that can process complex inputs and generate coherent, contextually appropriate responses in natural language, making them indispensable in many areas of technology and industry.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d931d7c-ba04-4a03-96bc-259b2c32e017",
   "metadata": {},
   "source": [
    "## tokenizer测试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184daaa2-8e7c-4c6d-8429-cf4f5a08194f",
   "metadata": {},
   "source": [
    "```\n",
    "[\n",
    "    {\"role\": \"user\", \"content\": \"Hi there!\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Nice to meet you!\"}\n",
    "]\n",
    "```\n",
    "此消息序列需要先转换为一个文本字符串，然后才能对其进行分词以输入给模型。但问题是，转换方法有很多！例如，你可以将消息列表转换为“即时消息”格式:\n",
    "\n",
    "```\n",
    "User: Hey there!\n",
    "Bot: Nice to meet you!\n",
    "```\n",
    "或者你可以添加特殊词元来指示角色:\n",
    "\n",
    "```\n",
    "[USER] Hey there! [/USER]\n",
    "[ASST] Nice to meet you! [/ASST]\n",
    "```\n",
    "抑或你可以添加词元以指示消息之间的边界，而将角色信息作为字符串插入:\n",
    "\n",
    "\n",
    "```\n",
    "<|im_start|>user\n",
    "Hey there!<|im_end|>\n",
    "<|im_start|>assistant\n",
    "Nice to meet you!<|im_end|>\n",
    "```\n",
    "方法多种多样，但没有哪种方法是最好的或是最正确的。因此，不同的模型会采用截然不同的格式进行训练。上面这些例子不是我编造的，它们都是真实的，并且至少被一个现存模型使用过！但是，一旦模型接受了某种格式的训练，你需要确保未来的输入使用相同的格式，否则就可能会出现损害性能的分布漂移。\n",
    "\n",
    "### 模板: 一种保存格式信息的方式\n",
    "当前的状况是: 如果幸运的话，你需要的格式已被正确记录在模型卡中的某个位置; 如果不幸的话，它不在，那如果你想用这个模型的话，只能祝你好运了; 在极端情况下，我们甚至会将整个提示格式放在 相应模型的博文 中，以确保用户不会错过它！但即使在最好的情况下，你也必须找到模板信息并在微调或推理流水线中手动将其写进代码。我们认为这是一个特别危险的做法，因为使用错误的聊天格式是一个 静默错误 - 一旦出了错，不会有显式的失败或 Python 异常来告诉你出了什么问题，模型的表现只会比用正确格式时差多了，但很难调试其原因！\n",
    "\n",
    "这正是 聊天模板 旨在解决的问题。聊天模板是一个 Jinja 模板字符串，你可以使用分词器保存和加载它。聊天模板包含了将聊天消息列表转换为模型所需的、格式正确的输入字符串所需要的全部信息。下面是三个聊天模板字符串，分别对应上文所述的三种消息格式:\n",
    "\n",
    "```\n",
    "{% for message in messages %}\n",
    "    {% if message['role'] == 'user' %}\n",
    "        {{ \"User : \" }}\n",
    "    {% else %}\n",
    "        {{ \"Bot : \" }}\n",
    "    {{ message['content'] + '\\n' }}\n",
    "{% endfor %}\n",
    "````\n",
    "\n",
    "```\n",
    "{% for message in messages %}\n",
    "    {% if message['role'] == 'user' %}\n",
    "        {{ \"[USER]\" + message['content'] + \" [/USER]\" }}\n",
    "    {% else %}\n",
    "        {{ \"[ASST]\" + message['content'] + \" [/ASST]\" }}\n",
    "    {{ message['content'] + '\\n' }}\n",
    "{% endfor %}\n",
    "```\n",
    "\n",
    "```\n",
    "\"{% for message in messages %}\"\n",
    "    \"{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}\"\n",
    "\"{% endfor %}\"\n",
    "```\n",
    "\n",
    "参考[聊天模板：无声性能杀手的终结](https://github.com/huggingface/blog/blob/main/zh/chat-templates.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e0739d9-b389-4673-8b2f-95db33ae071c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nHello, how are you?<|im_end|>\\n<|im_start|>assistant\\nI'm doing great. How can I help you today?<|im_end|>\\n<|im_start|>user\\nI'd like to show off how chat templating works!<|im_end|>\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\n",
    " \n",
    "chat = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n",
    "]\n",
    " \n",
    "tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False) # 用于训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1c724bd-7a18-49ce-b985-e1a88a76be21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nHello, how are you?<|im_end|>\\n<|im_start|>assistant\\nI'm doing great. How can I help you today?<|im_end|>\\n<|im_start|>user\\nI'd like to show off how chat templating works!<|im_end|>\\n<|im_start|>assistant\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True) # 用于推理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c201e135-558e-4194-b2c1-d56c5212c412",
   "metadata": {},
   "source": [
    "后者添加了模型开始答复的标记。这可以确保模型生成文本时只会给出答复，而不会做出意外的行为，比如继续用户的消息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cf15784-f29c-4b41-9edc-fef250a819d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- messages[0]['content'] }}\n",
      "    {%- else %}\n",
      "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
      "    {%- endif %}\n",
      "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
      "    {%- else %}\n",
      "        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {{- '<|im_start|>' + message.role }}\n",
      "        {%- if message.content %}\n",
      "            {{- '\\n' + message.content }}\n",
      "        {%- endif %}\n",
      "        {%- for tool_call in message.tool_calls %}\n",
      "            {%- if tool_call.function is defined %}\n",
      "                {%- set tool_call = tool_call.function %}\n",
      "            {%- endif %}\n",
      "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
      "            {{- tool_call.name }}\n",
      "            {{- '\", \"arguments\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- '}\\n</tool_call>' }}\n",
      "        {%- endfor %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "{%- endif %}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 查看现有的模板\n",
    "template = tokenizer.chat_template\n",
    "print(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9895e5d7-66c1-4c42-92ae-f4890a96f21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(template))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34feb425-4484-4dce-93c0-9c5484263f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改当前模板并且重新设置\n",
    "template = template.replace(\"You are Qwen, created by Alibaba Cloud.\", \"You are Qwen, created by Rongsheng.\")  # Change the system token\n",
    "tokenizer.chat_template = template  # Set the new template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd0ec395-956d-4310-b3ea-639da015ac87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- messages[0]['content'] }}\n",
      "    {%- else %}\n",
      "        {{- 'You are Qwen, created by Rongsheng. You are a helpful assistant.' }}\n",
      "    {%- endif %}\n",
      "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
      "    {%- else %}\n",
      "        {{- '<|im_start|>system\\nYou are Qwen, created by Rongsheng. You are a helpful assistant.<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {{- '<|im_start|>' + message.role }}\n",
      "        {%- if message.content %}\n",
      "            {{- '\\n' + message.content }}\n",
      "        {%- endif %}\n",
      "        {%- for tool_call in message.tool_calls %}\n",
      "            {%- if tool_call.function is defined %}\n",
      "                {%- set tool_call = tool_call.function %}\n",
      "            {%- endif %}\n",
      "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
      "            {{- tool_call.name }}\n",
      "            {{- '\", \"arguments\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- '}\\n</tool_call>' }}\n",
      "        {%- endfor %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "{%- endif %}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4e0c35-e317-4aeb-ad1d-10ed40efd41d",
   "metadata": {},
   "source": [
    "## 将chat_template添加到训练集中进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe62a079-7365-48f2-b114-ccb2cfdc09cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Collecting datasets\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/e3/f5/668b3444a2f487b0052b908af631fe39eeb2bdb2359d9bbc2c3b80b71119/datasets-3.5.1-py3-none-any.whl (491 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.24.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Collecting xxhash\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/f2/07/d9a3059f702dec5b3b703737afb6dda32f304f6e9da181a229dafd052c29/xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm>=4.66.3\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/d0/30/dc54f88dd4a2b5dc8a0279bdd7270e735851848b762aeb1c1184ed1f6b14/tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec[http]<=2025.3.0,>=2023.1.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (2024.6.0)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/lib/python3.10/site-packages (from datasets) (24.1)\n",
      "Collecting pyarrow>=15.0.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/f1/ca/ae10fba419a6e94329707487835ec721f5a95f3ac9168500bcf7aa3813c7/pyarrow-20.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (42.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Collecting pandas\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/44/50/7db2cd5e6373ae796f0ddad3675268c8d59fb6076e66f0c339d61cea886b/pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/e0/ab/40dacb15c0c58f7f17686ea67bc186e9f207341691bdb777d1d5ff4671d5/aiohttp-3.11.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess<0.70.17\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/bc/f7/7ec7fddc92e50714ea3745631f79bd9c96424cb2702632521028e57d3a36/multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.9,>=0.3.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/c9/7a/cef76fd8438a42f96db64ddaa85280485a9c395e7df3db8158cfec1eee34/dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.32.2 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.10/site-packages (from datasets) (3.14.0)\n",
      "Collecting async-timeout<6.0,>=4.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/fe/ba/e2081de779ca30d473f21f5b30e0e737c438205440784c7dfc81efc2b029/async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/8b/fd/10fcf7d86f49b1a11096d6846257485ef32e3d3d322e8a7fdea5b127880c/yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (333 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m333.9/333.9 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting propcache>=0.2.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/7c/e4/4aeb95a1cd085e0558ab0de95abfc5187329616193a1012a6c4c930e9f7a/propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.6/206.6 kB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohappyeyeballs>=2.3.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/0f/15/5bf3b99495fb160b63f95972b81750f18f7f4e02ad051373b669d17d44f2/aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/d0/6e/1b64671ab2fca1ebf32c5b500205724ac14c98b9bc1574b2ef55853f4d71/frozenlist-1.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.3/287.3 kB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/2c/e9/40d2b73e7d6574d91074d83477a990e3701affbe8b596010d4f5e6c7a6fa/multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (219 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.8/219.8 kB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/ec/6a/bc7e17a3e87a2985d3e8f4da4cd0f481060eb78fb08596c42be62c90a4d9/aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/81/c4/34e93fe5f5429d7570ec1fa436f1986fb1f00c3e0f43a589fe2bbcd22c3f/pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tzdata>=2022.7\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/5c/23/c7abc0ca0a1526a0774eca151daeb8de62ec457e77262b66b359c3c7679e/tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: pytz, xxhash, tzdata, tqdm, pyarrow, propcache, multidict, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, pandas, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.64.1\n",
      "    Uninstalling tqdm-4.64.1:\n",
      "      Successfully uninstalled tqdm-4.64.1\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 async-timeout-5.0.1 datasets-3.5.1 dill-0.3.8 frozenlist-1.6.0 multidict-6.4.3 multiprocess-0.70.16 pandas-2.2.3 propcache-0.3.1 pyarrow-20.0.0 pytz-2025.2 tqdm-4.67.1 tzdata-2025.2 xxhash-3.5.0 yarl-1.20.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b9c0d86-ad33-4a91-8104-1f8476017cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f4713c7144b4b00a1d673bb575ecaf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Which is bigger, the moon or the sun?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "The sun.<|im_end|>\n",
      "\n",
      "===============\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Which is bigger, a virus or a bacterium?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "A bacterium.<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\n",
    "\n",
    "chat1 = [\n",
    "    {\"role\": \"user\", \"content\": \"Which is bigger, the moon or the sun?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The sun.\"}\n",
    "]\n",
    "chat2 = [\n",
    "    {\"role\": \"user\", \"content\": \"Which is bigger, a virus or a bacterium?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"A bacterium.\"}\n",
    "]\n",
    "\n",
    "dataset = Dataset.from_dict({\"chat\": [chat1, chat2]})\n",
    "dataset = dataset.map(lambda x: {\"formatted_chat\": tokenizer.apply_chat_template(x[\"chat\"], tokenize=False, add_generation_prompt=False)})\n",
    "print(dataset['formatted_chat'][0])\n",
    "print(\"===============\")\n",
    "print(dataset['formatted_chat'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015ef60a-ecf0-4374-a86b-089b17d13b93",
   "metadata": {},
   "source": [
    "一些分词器（tokenizers）会添加特殊的 `<bos>` 和 `<eos>` 标记。聊天模板（Chat templates）中通常已经包含了所有必要的特殊标记，额外添加特殊标记通常是不正确或重复的，这可能会损害模型的性能。当你使用 `apply_chat_template(tokenize=False)` 来格式化文本时，请确保同时将 `add_special_tokens=False`，以避免重复添加这些特殊标记。\n",
    "\n",
    "参考[【搭建框架必备基础】彻底搞懂Chat template](https://www.guyuehome.com/detail?id=1888166611628642305)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e44bf44-62a4-49b1-8120-396e72d04074",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
